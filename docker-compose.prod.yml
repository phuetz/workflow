# ============================================
# WORKFLOW AUTOMATION PLATFORM
# Docker Compose - Production Environment
# ============================================

version: '3.9'

services:
  # ===========================================
  # Application (Production)
  # ===========================================
  app:
    build:
      context: .
      dockerfile: Dockerfile
      target: runner
      args:
        NODE_ENV: production
        VITE_API_BASE_URL: ${VITE_API_BASE_URL:-https://api.workflow-automation.com}
        VITE_GRAPHQL_URL: ${VITE_GRAPHQL_URL:-https://api.workflow-automation.com/graphql}
        VITE_WS_URL: ${VITE_WS_URL:-wss://api.workflow-automation.com/ws}
    image: workflow-app:${VERSION:-latest}
    container_name: workflow-app
    restart: always
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 5s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    ports:
      - target: 3001
        published: 3001
        protocol: tcp
        mode: ingress
    environment:
      - NODE_ENV=production
      - PORT=3001
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - JWT_SECRET=${JWT_SECRET}
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - METRICS_ENABLED=true
      - TRACING_ENABLED=true
    volumes:
      - app-logs:/app/logs:rw
      - app-uploads:/app/uploads:rw
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - workflow-prod
      - monitoring
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service=app,environment=production"

  # ===========================================
  # PostgreSQL Database (Production)
  # ===========================================
  postgres:
    image: postgres:15-alpine
    container_name: workflow-postgres
    restart: always
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-workflow}
      POSTGRES_USER: ${POSTGRES_USER:-workflow}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
      PGDATA: /var/lib/postgresql/data/pgdata
    ports:
      - "127.0.0.1:5432:5432"  # Bind to localhost only
    volumes:
      - postgres-prod-data:/var/lib/postgresql/data
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro
      - postgres-backups:/backups
    networks:
      - workflow-prod
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-workflow} -d ${POSTGRES_DB:-workflow}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================
  # Redis Cache (Production)
  # ===========================================
  redis:
    image: redis:7-alpine
    container_name: workflow-redis
    restart: always
    command: >
      redis-server
      --appendonly yes
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    ports:
      - "127.0.0.1:6379:6379"  # Bind to localhost only
    volumes:
      - redis-prod-data:/data
    networks:
      - workflow-prod
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 512M
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================
  # NGINX Reverse Proxy
  # ===========================================
  nginx:
    image: nginx:alpine
    container_name: workflow-nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./docker/nginx/conf.d:/etc/nginx/conf.d:ro
      - ./docker/nginx/ssl:/etc/nginx/ssl:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    depends_on:
      - app
    networks:
      - workflow-prod
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  # ===========================================
  # Prometheus (Monitoring)
  # ===========================================
  prometheus:
    image: prom/prometheus:latest
    container_name: workflow-prometheus
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "127.0.0.1:9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./docker/prometheus/alerts:/etc/prometheus/alerts:ro
      - prometheus-data:/prometheus
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================
  # Grafana (Visualization)
  # ===========================================
  grafana:
    image: grafana/grafana:latest
    container_name: workflow-grafana
    restart: always
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=https://grafana.workflow-automation.com
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    ports:
      - "127.0.0.1:3003:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./docker/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./docker/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ===========================================
  # Node Exporter (System metrics)
  # ===========================================
  node-exporter:
    image: prom/node-exporter:latest
    container_name: workflow-node-exporter
    restart: always
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 128M
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "2"

  # ===========================================
  # Backup Service
  # ===========================================
  backup:
    image: alpine:latest
    container_name: workflow-backup
    restart: "no"
    volumes:
      - postgres-prod-data:/backup/postgres:ro
      - redis-prod-data:/backup/redis:ro
      - app-uploads:/backup/uploads:ro
      - postgres-backups:/backups:rw
      - ./docker/scripts/backup.sh:/backup.sh:ro
    environment:
      - BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-0 2 * * *}
      - BACKUP_RETENTION_DAYS=${BACKUP_RETENTION_DAYS:-30}
      - S3_BUCKET=${BACKUP_S3_BUCKET}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    networks:
      - workflow-prod
    entrypoint: ["/bin/sh", "-c"]
    command: ["crond -f -l 2"]
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M

networks:
  workflow-prod:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.30.0.0/16
  monitoring:
    driver: bridge

volumes:
  postgres-prod-data:
    driver: local
  redis-prod-data:
    driver: local
  postgres-backups:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  app-logs:
    driver: local
  app-uploads:
    driver: local
  nginx-cache:
    driver: local
  nginx-logs:
    driver: local

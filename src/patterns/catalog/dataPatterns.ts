/**
 * Data Patterns (10 patterns)
 * Data processing and transformation patterns
 */

import type { PatternDefinition } from '../../types/patterns';
import {
  createPatternDefinition,
  createPatternStructure,
  createEdgePattern,
  PatternConstraints,
} from '../PatternDefinition';

export const ETL: PatternDefinition = createPatternDefinition({
  id: 'etl',
  name: 'ETL (Extract, Transform, Load)',
  category: 'data',
  complexity: 'intermediate',
  description: 'Extract data from sources, transform it, and load into destination',
  problem: 'Need to move and transform data between systems',
  solution: 'Three-phase process: extract from source, transform data, load to destination',
  benefits: [
    'Data consolidation',
    'Data quality improvement',
    'Business intelligence',
    'Data warehousing',
  ],
  tradeoffs: [
    'Processing time',
    'Resource intensive',
    'Complex transformations',
    'Maintenance overhead',
  ],
  useCases: [
    'Data warehousing',
    'Database migration',
    'Data analytics',
    'System integration',
  ],
  tags: ['data', 'etl', 'transformation', 'integration'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['http-request', 'function', 'database'],
    optionalNodeTypes: ['filter', 'aggregate'],
    requiredEdges: [
      createEdgePattern('extract', 'transform', 'sequential'),
      createEdgePattern('transform', 'load', 'sequential'),
    ],
    topology: 'linear',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['no-validation', 'no-error-handling'],
  relatedPatterns: ['elt', 'data-pipeline'],
  documentation: 'Extracts, transforms, and loads data between systems.',
});

export const DATA_VALIDATION: PatternDefinition = createPatternDefinition({
  id: 'data-validation',
  name: 'Data Validation',
  category: 'data',
  complexity: 'beginner',
  description: 'Verify data meets requirements before processing',
  problem: 'Invalid data can cause processing errors and data corruption',
  solution: 'Validate all inputs against schema and business rules',
  benefits: [
    'Data quality',
    'Early error detection',
    'Data integrity',
    'Compliance',
  ],
  tradeoffs: [
    'Processing overhead',
    'Validation rule maintenance',
    'False positives',
    'Complexity',
  ],
  useCases: [
    'API input validation',
    'Form validation',
    'Data import',
    'ETL pipelines',
  ],
  tags: ['data', 'validation', 'quality', 'integrity'],
  structure: createPatternStructure({
    minNodes: 2,
    requiredNodeTypes: ['filter', 'switch'],
    optionalNodeTypes: ['function'],
    requiredEdges: [createEdgePattern('validate', 'accept-reject', 'conditional')],
    topology: 'branching',
    constraints: [PatternConstraints.nodeCount(2)],
  }),
  examples: [],
  antiPatterns: ['no-validation', 'client-side-only'],
  relatedPatterns: ['schema-validation', 'sanitization'],
  documentation: 'Validates data against schema and business rules.',
});

export const DATA_ENRICHMENT: PatternDefinition = createPatternDefinition({
  id: 'data-enrichment',
  name: 'Data Enrichment',
  category: 'data',
  complexity: 'intermediate',
  description: 'Enhance data by adding information from external sources',
  problem: 'Data is incomplete or lacks context',
  solution: 'Lookup and merge additional data from other sources',
  benefits: [
    'Complete data',
    'Better insights',
    'Improved quality',
    'Added value',
  ],
  tradeoffs: [
    'External dependencies',
    'Processing time',
    'Cost of lookups',
    'Synchronization',
  ],
  useCases: [
    'Customer data enrichment',
    'Geolocation lookup',
    'Price enrichment',
    'Analytics enhancement',
  ],
  tags: ['data', 'enrichment', 'augmentation', 'lookup'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['http-request', 'merge'],
    optionalNodeTypes: ['cache', 'database'],
    requiredEdges: [createEdgePattern('lookup', 'merge', 'sequential')],
    topology: 'dag',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['missing-cache', 'synchronous-lookup'],
  relatedPatterns: ['cache-aside', 'lookup'],
  documentation: 'Enriches data by adding information from external sources.',
});

export const DATA_AGGREGATION: PatternDefinition = createPatternDefinition({
  id: 'data-aggregation',
  name: 'Data Aggregation',
  category: 'data',
  complexity: 'intermediate',
  description: 'Combine data from multiple sources into unified view',
  problem: 'Data is scattered across multiple sources',
  solution: 'Collect and combine data into aggregated format',
  benefits: [
    'Unified view',
    'Simplified access',
    'Better analysis',
    'Reduced queries',
  ],
  tradeoffs: [
    'Aggregation complexity',
    'Staleness',
    'Storage requirements',
    'Maintenance',
  ],
  useCases: [
    'Dashboard data',
    'Reporting',
    'Analytics',
    'Summary views',
  ],
  tags: ['data', 'aggregation', 'combination', 'summary'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['http-request', 'aggregate'],
    optionalNodeTypes: ['merge', 'function'],
    requiredEdges: [createEdgePattern('fetch', 'aggregate', 'sequential')],
    topology: 'dag',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['real-time-only', 'no-caching'],
  relatedPatterns: ['scatter-gather', 'merge'],
  documentation: 'Combines data from multiple sources into unified view.',
});

export const CACHE_ASIDE: PatternDefinition = createPatternDefinition({
  id: 'cache-aside',
  name: 'Cache-Aside',
  category: 'data',
  complexity: 'intermediate',
  description: 'Load data into cache on demand',
  problem: 'Database queries are slow and repetitive',
  solution: 'Check cache first, load from database on miss, update cache',
  benefits: [
    'Improved performance',
    'Reduced database load',
    'On-demand loading',
    'Simple implementation',
  ],
  tradeoffs: [
    'Cache invalidation',
    'Stale data',
    'Cache warming',
    'Consistency challenges',
  ],
  useCases: [
    'Database caching',
    'API response caching',
    'Session data',
    'Configuration',
  ],
  tags: ['data', 'cache', 'performance', 'lazy-loading'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['cache', 'database', 'switch'],
    optionalNodeTypes: ['http-request'],
    requiredEdges: [
      createEdgePattern('check-cache', 'hit-miss', 'conditional'),
      createEdgePattern('miss', 'load-update-cache', 'sequential'),
    ],
    topology: 'branching',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['no-cache-invalidation', 'cache-stampede'],
  relatedPatterns: ['cache-through', 'cache-behind'],
  documentation: 'Loads data into cache on demand from database.',
});

export const DATA_PARTITIONING: PatternDefinition = createPatternDefinition({
  id: 'data-partitioning',
  name: 'Data Partitioning',
  category: 'data',
  complexity: 'advanced',
  description: 'Divide data into partitions for scalability and performance',
  problem: 'Single database cannot handle all data or load',
  solution: 'Split data across multiple partitions based on key',
  benefits: [
    'Horizontal scaling',
    'Improved performance',
    'Parallel processing',
    'Fault isolation',
  ],
  tradeoffs: [
    'Complex queries',
    'Rebalancing',
    'Cross-partition queries',
    'Partition key design',
  ],
  useCases: [
    'Large databases',
    'Multi-tenant systems',
    'Time-series data',
    'Geographic distribution',
  ],
  tags: ['data', 'partitioning', 'sharding', 'scalability'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['switch', 'database'],
    optionalNodeTypes: ['hash', 'function'],
    requiredEdges: [createEdgePattern('route', 'partition', 'conditional')],
    topology: 'branching',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['hot-partitions', 'poor-partition-key'],
  relatedPatterns: ['sharding', 'cqrs'],
  documentation: 'Divides data into partitions for scalability.',
});

export const CQRS: PatternDefinition = createPatternDefinition({
  id: 'cqrs',
  name: 'CQRS (Command Query Responsibility Segregation)',
  category: 'data',
  complexity: 'expert',
  description: 'Separate read and write models for better performance and scalability',
  problem: 'Single model for reads and writes causes performance issues',
  solution: 'Use different models optimized for commands (writes) and queries (reads)',
  benefits: [
    'Performance optimization',
    'Independent scaling',
    'Simplified models',
    'Better security',
  ],
  tradeoffs: [
    'Eventual consistency',
    'Complexity',
    'Synchronization',
    'Duplicate code',
  ],
  useCases: [
    'High-traffic systems',
    'Complex domains',
    'Different read/write patterns',
    'Event sourcing',
  ],
  tags: ['data', 'cqrs', 'separation', 'scalability'],
  structure: createPatternStructure({
    minNodes: 4,
    requiredNodeTypes: ['switch', 'database'],
    optionalNodeTypes: ['event-bus', 'cache'],
    requiredEdges: [createEdgePattern('route', 'read-write-model', 'conditional')],
    topology: 'branching',
    constraints: [PatternConstraints.nodeCount(4)],
  }),
  examples: [],
  antiPatterns: ['shared-model', 'synchronous-sync'],
  relatedPatterns: ['event-sourcing', 'materialized-view'],
  documentation: 'Separates read and write models for optimization.',
});

export const EVENT_SOURCING: PatternDefinition = createPatternDefinition({
  id: 'event-sourcing',
  name: 'Event Sourcing',
  category: 'data',
  complexity: 'expert',
  description: 'Store state changes as sequence of events',
  problem: 'Current state only, no history or audit trail',
  solution: 'Store all changes as events, rebuild state by replaying events',
  benefits: [
    'Complete audit trail',
    'Temporal queries',
    'Event replay',
    'Debugging',
  ],
  tradeoffs: [
    'Complexity',
    'Storage requirements',
    'Event versioning',
    'Query complexity',
  ],
  useCases: [
    'Audit requirements',
    'Temporal analysis',
    'CQRS systems',
    'Collaboration',
  ],
  tags: ['data', 'event-sourcing', 'audit', 'history'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['event-store', 'database'],
    optionalNodeTypes: ['function'],
    requiredEdges: [createEdgePattern('event', 'store', 'sequential')],
    topology: 'linear',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['mutable-events', 'no-versioning'],
  relatedPatterns: ['cqrs', 'event-driven'],
  documentation: 'Stores all state changes as immutable events.',
});

export const MATERIALIZED_VIEW: PatternDefinition = createPatternDefinition({
  id: 'materialized-view',
  name: 'Materialized View',
  category: 'data',
  complexity: 'intermediate',
  description: 'Pre-compute and store query results for fast access',
  problem: 'Complex queries are slow to execute repeatedly',
  solution: 'Pre-compute results and store as materialized view',
  benefits: [
    'Fast queries',
    'Reduced computation',
    'Better performance',
    'Simplified queries',
  ],
  tradeoffs: [
    'Storage requirements',
    'Refresh overhead',
    'Staleness',
    'Maintenance',
  ],
  useCases: [
    'Reporting',
    'Dashboards',
    'Analytics',
    'Complex aggregations',
  ],
  tags: ['data', 'view', 'performance', 'pre-computation'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['database', 'aggregate'],
    optionalNodeTypes: ['schedule', 'cache'],
    requiredEdges: [createEdgePattern('compute', 'store', 'sequential')],
    topology: 'linear',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['never-refresh', 'over-materialization'],
  relatedPatterns: ['cqrs', 'cache-aside'],
  documentation: 'Pre-computes and stores query results for fast access.',
});

export const CHANGE_DATA_CAPTURE: PatternDefinition = createPatternDefinition({
  id: 'change-data-capture',
  name: 'Change Data Capture (CDC)',
  category: 'data',
  complexity: 'advanced',
  description: 'Capture and propagate database changes in real-time',
  problem: 'Need to sync data changes across systems in real-time',
  solution: 'Monitor database transaction log and emit change events',
  benefits: [
    'Real-time sync',
    'No application changes',
    'Complete capture',
    'Low overhead',
  ],
  tradeoffs: [
    'Database-specific',
    'Complex setup',
    'Event ordering',
    'Schema changes',
  ],
  useCases: [
    'Database replication',
    'Cache invalidation',
    'Search index updates',
    'Event-driven architecture',
  ],
  tags: ['data', 'cdc', 'real-time', 'sync'],
  structure: createPatternStructure({
    minNodes: 3,
    requiredNodeTypes: ['database', 'webhook'],
    optionalNodeTypes: ['kafka', 'event-bus'],
    requiredEdges: [createEdgePattern('capture', 'emit', 'sequential')],
    topology: 'linear',
    constraints: [PatternConstraints.nodeCount(3)],
  }),
  examples: [],
  antiPatterns: ['polling-for-changes', 'trigger-based-cdc'],
  relatedPatterns: ['event-sourcing', 'event-driven'],
  documentation: 'Captures database changes and emits events in real-time.',
});

/**
 * All data patterns
 */
export const DATA_PATTERNS: PatternDefinition[] = [
  ETL,
  DATA_VALIDATION,
  DATA_ENRICHMENT,
  DATA_AGGREGATION,
  CACHE_ASIDE,
  DATA_PARTITIONING,
  CQRS,
  EVENT_SOURCING,
  MATERIALIZED_VIEW,
  CHANGE_DATA_CAPTURE,
];
